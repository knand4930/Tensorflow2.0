{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cancer_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "0                  0.2654          0.4601                  0.11890   \n",
       "1                  0.1860          0.2750                  0.08902   \n",
       "2                  0.2430          0.3613                  0.08758   \n",
       "3                  0.2575          0.6638                  0.17300   \n",
       "4                  0.1625          0.2364                  0.07678   \n",
       "..                    ...             ...                      ...   \n",
       "564                0.2216          0.2060                  0.07115   \n",
       "565                0.1628          0.2572                  0.06637   \n",
       "566                0.1418          0.2218                  0.07820   \n",
       "567                0.2650          0.4087                  0.12400   \n",
       "568                0.0000          0.2871                  0.07039   \n",
       "\n",
       "     benign_0__mal_1  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "564                0  \n",
       "565                0  \n",
       "566                0  \n",
       "567                0  \n",
       "568                1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\machine learning\\\\tensorflow_2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datatime \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-10-08'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = \"logs\\\\fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Embeddings will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    }
   ],
   "source": [
    "board = TensorBoard(log_dir= log_directory,histogram_freq=1,\n",
    "                   write_graph=True,\n",
    "                   write_images = True,\n",
    "                   update_freq='epoch',\n",
    "                   profile_batch=2,\n",
    "                   embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1,activation='relu'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 2s 5ms/sample - loss: 6.5048 - val_loss: 2.2979\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 4.9834 - val_loss: 1.2995\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 285us/sample - loss: 3.4617 - val_loss: 1.0225\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 357us/sample - loss: 3.7647 - val_loss: 0.9143\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 287us/sample - loss: 3.0853 - val_loss: 0.8606\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 2.3952 - val_loss: 0.8233\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 337us/sample - loss: 2.6374 - val_loss: 0.9001\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 355us/sample - loss: 2.4143 - val_loss: 0.8986\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 334us/sample - loss: 2.1661 - val_loss: 0.8682\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 358us/sample - loss: 1.9897 - val_loss: 0.7739\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 407us/sample - loss: 1.9765 - val_loss: 0.7555\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 2.0204 - val_loss: 0.7542\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 1.6980 - val_loss: 0.7456\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 1.7906 - val_loss: 0.7369\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 255us/sample - loss: 1.5290 - val_loss: 0.7220\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 1.7176 - val_loss: 0.7030\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 1.5954 - val_loss: 0.6859\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 308us/sample - loss: 1.1394 - val_loss: 0.6677\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 1.4952 - val_loss: 0.6620\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 259us/sample - loss: 1.2732 - val_loss: 0.6726\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 257us/sample - loss: 1.2054 - val_loss: 0.6799\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 1.3139 - val_loss: 0.6699\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 357us/sample - loss: 1.1264 - val_loss: 0.6427\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 278us/sample - loss: 1.4899 - val_loss: 0.6205\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 1.1324 - val_loss: 0.6517\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 358us/sample - loss: 0.9041 - val_loss: 0.6437\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 332us/sample - loss: 1.0389 - val_loss: 0.6128\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.9364 - val_loss: 0.5853\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 1.0716 - val_loss: 0.5727\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 307us/sample - loss: 0.9509 - val_loss: 0.5533\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 327us/sample - loss: 1.0710 - val_loss: 0.5397\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 337us/sample - loss: 1.0299 - val_loss: 0.5240\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.9196 - val_loss: 0.5159\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 304us/sample - loss: 0.9137 - val_loss: 0.5140\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 326us/sample - loss: 0.7532 - val_loss: 0.4947\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 326us/sample - loss: 0.8411 - val_loss: 0.4753\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.7891 - val_loss: 0.4773\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 648us/sample - loss: 0.7304 - val_loss: 0.4670\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 359us/sample - loss: 0.8473 - val_loss: 0.4460\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 327us/sample - loss: 0.8096 - val_loss: 0.4237\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 0.7883 - val_loss: 0.4748\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 337us/sample - loss: 0.7665 - val_loss: 0.6748\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 1.0942 - val_loss: 0.6687\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.8672 - val_loss: 0.6219\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 438us/sample - loss: 0.8007 - val_loss: 0.5700\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 334us/sample - loss: 1.0399 - val_loss: 0.5588\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 337us/sample - loss: 0.8119 - val_loss: 0.5474\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 336us/sample - loss: 0.8607 - val_loss: 0.5267\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.8258 - val_loss: 0.5067\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.8375 - val_loss: 0.4913\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 518us/sample - loss: 0.7794 - val_loss: 0.4723\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 360us/sample - loss: 0.8143 - val_loss: 0.4683\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.7717 - val_loss: 0.4492\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 336us/sample - loss: 0.8418 - val_loss: 0.4838\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 339us/sample - loss: 0.6603 - val_loss: 0.5845\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 365us/sample - loss: 0.7260 - val_loss: 0.5632\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 301us/sample - loss: 0.7038 - val_loss: 0.5125\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 300us/sample - loss: 0.7333 - val_loss: 0.4735\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 299us/sample - loss: 0.7651 - val_loss: 0.4395\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.7452 - val_loss: 0.4287\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.6193 - val_loss: 0.4196\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 304us/sample - loss: 0.6404 - val_loss: 0.4157\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 294us/sample - loss: 0.6705 - val_loss: 0.3996\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 301us/sample - loss: 0.6483 - val_loss: 0.3841\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.6282 - val_loss: 0.3847\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.6021 - val_loss: 0.3829\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 302us/sample - loss: 0.6697 - val_loss: 0.3774\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.6894 - val_loss: 0.3703\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.5716 - val_loss: 0.3628\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.6554 - val_loss: 0.3459\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 288us/sample - loss: 0.6733 - val_loss: 0.3237\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 271us/sample - loss: 0.5860 - val_loss: 0.3186\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.6569 - val_loss: 0.3174\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 309us/sample - loss: 0.6482 - val_loss: 0.3055\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.5472 - val_loss: 0.2931\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.6912 - val_loss: 0.2931\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 306us/sample - loss: 0.6121 - val_loss: 0.2980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 641us/sample - loss: 0.5963 - val_loss: 0.2964\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 257us/sample - loss: 0.6455 - val_loss: 0.2829\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 474us/sample - loss: 0.6874 - val_loss: 0.2703\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.5086 - val_loss: 0.2828\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.6304 - val_loss: 0.2753\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.6221 - val_loss: 0.3346\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.6638 - val_loss: 0.3222\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.4906 - val_loss: 0.2935\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.5806 - val_loss: 0.2809\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.5416 - val_loss: 0.2674\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.6316 - val_loss: 0.2628\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.6055 - val_loss: 0.2540\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 247us/sample - loss: 0.5195 - val_loss: 0.2671\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 257us/sample - loss: 0.5417 - val_loss: 0.2601\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 227us/sample - loss: 0.5741 - val_loss: 0.2425\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.6097 - val_loss: 0.2335\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 342us/sample - loss: 0.5666 - val_loss: 0.2233\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.5830 - val_loss: 0.2200\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.5485 - val_loss: 0.2243\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 224us/sample - loss: 0.5923 - val_loss: 0.2250\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 254us/sample - loss: 0.4585 - val_loss: 0.3008\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.5271 - val_loss: 0.3116\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.4469 - val_loss: 0.2879\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.6697 - val_loss: 0.2701\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.4456 - val_loss: 0.2566\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.5458 - val_loss: 0.2483\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 250us/sample - loss: 0.5317 - val_loss: 0.2351\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 250us/sample - loss: 0.3950 - val_loss: 0.2249\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.5371 - val_loss: 0.2168\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 257us/sample - loss: 0.5036 - val_loss: 0.2083\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.4383 - val_loss: 0.2090\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.6272 - val_loss: 0.2044\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 248us/sample - loss: 0.3249 - val_loss: 0.1980\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.5410 - val_loss: 0.1954\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.4609 - val_loss: 0.2014\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.4148 - val_loss: 0.1930\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.4102 - val_loss: 0.1865\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 254us/sample - loss: 0.4115 - val_loss: 0.1819\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.5814 - val_loss: 0.1878\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.4802 - val_loss: 0.2446\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 664us/sample - loss: 0.4329 - val_loss: 0.3139\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 442us/sample - loss: 0.5297 - val_loss: 0.2852\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 328us/sample - loss: 0.4483 - val_loss: 0.2551\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 305us/sample - loss: 0.4358 - val_loss: 0.2376\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.5047 - val_loss: 0.2144\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 308us/sample - loss: 0.5126 - val_loss: 0.2131\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.5247 - val_loss: 0.2459\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 338us/sample - loss: 0.4638 - val_loss: 0.2485\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.5777 - val_loss: 0.2418\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 301us/sample - loss: 0.5178 - val_loss: 0.2729\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 306us/sample - loss: 0.4554 - val_loss: 0.2815\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 316us/sample - loss: 0.4742 - val_loss: 0.2601\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 304us/sample - loss: 0.3536 - val_loss: 0.2375\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 328us/sample - loss: 0.4832 - val_loss: 0.2152\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 327us/sample - loss: 0.4333 - val_loss: 0.2096\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.3959 - val_loss: 0.2087\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.3848 - val_loss: 0.2013\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.3804 - val_loss: 0.1952\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.5371 - val_loss: 0.1900\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.4516 - val_loss: 0.1863\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 304us/sample - loss: 0.6203 - val_loss: 0.2025\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 305us/sample - loss: 0.5091 - val_loss: 0.2192\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 309us/sample - loss: 0.4257 - val_loss: 0.2084\n",
      "Epoch 00140: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18da1146348>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= X_train, y=y_train, epochs=600,validation_data=(X_test,y_test), verbose=1, callbacks=[early_stop, board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs\\fit\n"
     ]
    }
   ],
   "source": [
    "print(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\machine learning\\\\tensorflow_2'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-d0128b680960>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-40-d0128b680960>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Tensorboard --logdir logs\\\\fit\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Tensorboard --logdir logs\\\\fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
